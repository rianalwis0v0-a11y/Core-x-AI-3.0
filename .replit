modules = ["nodejs-20", "web", "postgresql-16"]
run = "npm run dev"
hidden = [".config", ".git", "generated-icon.png", "node_modules", "dist"]

[nix]
channel = "stable-24_05"

[deployment]
deploymentTarget = "autoscale"
build = ["npm", "run", "build"]
run = ["npm", "run", "start"]

[[ports]]
localPort = 5000
externalPort = 80

[[ports]]
localPort = 40097
externalPort = 3000

[env]
PORT = "5000"

[workflows]
runButton = "Start App with Ollama"

[[workflows.workflow]]
name = "Project"
mode = "parallel"
author = "agent"

[[workflows.workflow.tasks]]
task = "workflow.run"
args = "Start application"

[[workflows.workflow]]
name = "Start application"
author = "agent"

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "npm run dev"
waitForPort = 5000

[[workflows.workflow]]
name = "Start Ollama"
author = 47808259
mode = "sequential"

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "$HOME/.ollama/bin/ollama serve"

[[workflows.workflow]]
name = "Download Llama Model"
author = 47808259
mode = "sequential"

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "$HOME/.ollama/bin/ollama pull llama3.2:3b"

[[workflows.workflow]]
name = "Start App with Ollama"
author = 47808259
mode = "parallel"

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "$HOME/.ollama/bin/ollama serve"

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "npm run dev"

[agent]
integrations = ["javascript_mem_db:1.0.0", "javascript_openai:1.0.0"]
